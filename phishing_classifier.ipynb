{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNTabm1rHeg2qNgNoi/gnoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f9812e9f4d964c1eb33365ade86cb799":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36648f68db1542bf890e33f0c48e9497","IPY_MODEL_aa5fb1c7ff2f4779991da0a2336dd0e8","IPY_MODEL_91c78c8480b742eabbd7c093a1bd4347"],"layout":"IPY_MODEL_db4137a409ca487f97018b6b98f3fbe8"}},"36648f68db1542bf890e33f0c48e9497":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f97cc38e4970428d9cbcc604c9b5d449","placeholder":"​","style":"IPY_MODEL_828dd5d1fde646e3a100c3bd38e68514","value":"Map: 100%"}},"aa5fb1c7ff2f4779991da0a2336dd0e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8da87bc0b4c8415b84e8e7b84d063ae1","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_852ccb25723b4f0ca1a1e1080306d04b","value":450}},"91c78c8480b742eabbd7c093a1bd4347":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f865a45c28f480c8d4dbb21c980fac0","placeholder":"​","style":"IPY_MODEL_9858bbe03c424c3cab3be3f746451276","value":" 450/450 [00:00&lt;00:00, 7616.65 examples/s]"}},"db4137a409ca487f97018b6b98f3fbe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f97cc38e4970428d9cbcc604c9b5d449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828dd5d1fde646e3a100c3bd38e68514":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8da87bc0b4c8415b84e8e7b84d063ae1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"852ccb25723b4f0ca1a1e1080306d04b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f865a45c28f480c8d4dbb21c980fac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9858bbe03c424c3cab3be3f746451276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install Required Packages\n","\n","In order to run this notebook, you need to have several Python packages installed. The commands below will install the following packages:\n","\n","- **datasets:** Provides access to a wide range of datasets and tools to load and process them.\n","- **transformers:** Contains pre-trained models and tools for working with transformer-based architectures.\n","- **evaluate:** Offers utilities to compute evaluation metrics.\n","- **numpy:** A fundamental package for numerical computing in Python.\n","- **codecarbon:** Tracks energy consumption and emissions during model training.\n","\n"],"metadata":{"id":"Xf_MuzeFj08T"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n","!pip install evaluate\n","!pip install numpy\n","!pip install codecarbon"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XziwJBzhi-p","executionInfo":{"status":"ok","timestamp":1742933459921,"user_tz":-330,"elapsed":11964,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"3827c922-3316-4f68-a28d-ed8abd3792d8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: codecarbon in /usr/local/lib/python3.11/dist-packages (2.8.3)\n","Requirement already satisfied: arrow in /usr/local/lib/python3.11/dist-packages (from codecarbon) (1.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n","Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.20.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.2.2)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n","Requirement already satisfied: questionary in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.1.0)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from codecarbon) (3.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.32.3)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n","Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.2)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.8.2)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.9.0.20241206)\n","Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n","Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n","Requirement already satisfied: yaspin in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (3.1.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n","Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n","Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2025.1.31)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (4.12.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n","Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n","Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n"]}]},{"cell_type":"markdown","source":["# Import Necessary Libraries\n","\n","This cell imports all the required libraries for loading datasets, preprocessing, model training, evaluation, and energy consumption tracking. Here's what each import does:\n","\n","- **load_dataset (from datasets):**  \n","  - Loads datasets from Hugging Face's Datasets library. It supports a variety of datasets for natural language processing (NLP) tasks.\n","\n","- **AutoTokenizer, AutoModelForSequenceClassification (from transformers):**  \n","  - `AutoTokenizer` automatically loads the appropriate tokenizer for a given model.\n","  - `AutoModelForSequenceClassification` loads a pretrained transformer model designed for text classification.\n","\n","- **TrainingArguments, Trainer (from transformers):**  \n","  - `TrainingArguments` allows you to configure parameters like batch size, learning rate, and number of epochs.\n","  - `Trainer` handles the training and evaluation processes using the Hugging Face Transformers library.\n","\n","- **evaluate:**  \n","  - Provides tools for computing evaluation metrics such as accuracy, F1-score, or recall.\n","\n","- **numpy:**  \n","  - Used for efficient numerical operations and matrix manipulations.\n","\n","- **DataCollatorWithPadding (from transformers):**  \n","  - Automatically pads sequences to the maximum length in a batch, ensuring consistent input size for the model.\n","\n","- **EmissionsTracker (from codecarbon):**  \n","  - Tracks the carbon footprint of the training process, providing insights into energy consumption and CO₂ emissions.\n","\n","These imports are essential for loading data, building models, training with transformers, evaluating results, and tracking environmental impact.\n","\n"],"metadata":{"id":"9BqRK0YFkAUy"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import evaluate\n","import numpy as np\n","from transformers import DataCollatorWithPadding\n","from codecarbon import EmissionsTracker"],"metadata":{"id":"xcNSVsGThe9_","executionInfo":{"status":"ok","timestamp":1742933460013,"user_tz":-330,"elapsed":76,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Load the Dataset\n","\n","This line of code loads the **Phishing Site Classification** dataset using the Hugging Face Datasets library.\n","\n","***About the Dataset:***\n","\n","- **Dataset Name:** Phishing Site Classification\n","- **Source:** Hugging Face (Published by Shawhin)\n","- **Task:** Binary Classification — Classify URLs as either Safe or Not Safe.\n","- **Objective:** The dataset is specifically designed to train and evaluate machine learning models for detecting phishing sites."],"metadata":{"id":"2z7PqikDkLNN"}},{"cell_type":"code","source":["dataset_dict = load_dataset(\"shawhin/phishing-site-classification\")"],"metadata":{"id":"epud7_YRiHUQ","executionInfo":{"status":"ok","timestamp":1742933462912,"user_tz":-330,"elapsed":2896,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e288b2b4-7d60-4200-9bd6-fbab0fc6e78c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## View the Loaded Dataset\n","\n","\n"],"metadata":{"id":"6B3cYzcjkTln"}},{"cell_type":"code","source":["dataset_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwRSkD6_ipBz","executionInfo":{"status":"ok","timestamp":1742933462921,"user_tz":-330,"elapsed":7,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"caafa104-fb20-431c-e3e8-dca51323f4a7"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 2100\n","    })\n","    validation: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 450\n","    })\n","    test: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 450\n","    })\n","})"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# Load Pretrained BERT Model and Tokenizer\n","\n","This section of code loads a pretrained **BERT (Bidirectional Encoder Representations from Transformers)** model and its tokenizer using Hugging Face's `transformers` library.\n"],"metadata":{"id":"PNu5ZoUNkgXP"}},{"cell_type":"code","source":["# Load model directly\n","model_path = \"google-bert/bert-base-uncased\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","id2label = {0: \"Safe\", 1: \"Not Safe\"}\n","label2id = {\"Safe\": 0, \"Not Safe\": 1}\n","model = AutoModelForSequenceClassification.from_pretrained(model_path,\n","                                                           num_labels=2,\n","                                                           id2label=id2label,\n","                                                           label2id=label2id,)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8E0BhimjCgZ","executionInfo":{"status":"ok","timestamp":1742933463603,"user_tz":-330,"elapsed":680,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"63c49ebe-70f2-444e-f21d-4f2de8d5e055"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["# Print Model Layers and Trainable Parameters\n","\n","This code iterates through all the parameters of the model and prints their names along with their `requires_grad` status.\n","\n"],"metadata":{"id":"pjxsA9LTko0w"}},{"cell_type":"code","source":["# print layers\n","for name, param in model.named_parameters():\n","   print(name, param.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BurH3KQFjFaf","executionInfo":{"status":"ok","timestamp":1742933463622,"user_tz":-330,"elapsed":16,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"4a4dc0a8-8f83-4462-8d3f-89ab8ccc4b37"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["bert.embeddings.word_embeddings.weight True\n","bert.embeddings.position_embeddings.weight True\n","bert.embeddings.token_type_embeddings.weight True\n","bert.embeddings.LayerNorm.weight True\n","bert.embeddings.LayerNorm.bias True\n","bert.encoder.layer.0.attention.self.query.weight True\n","bert.encoder.layer.0.attention.self.query.bias True\n","bert.encoder.layer.0.attention.self.key.weight True\n","bert.encoder.layer.0.attention.self.key.bias True\n","bert.encoder.layer.0.attention.self.value.weight True\n","bert.encoder.layer.0.attention.self.value.bias True\n","bert.encoder.layer.0.attention.output.dense.weight True\n","bert.encoder.layer.0.attention.output.dense.bias True\n","bert.encoder.layer.0.attention.output.LayerNorm.weight True\n","bert.encoder.layer.0.attention.output.LayerNorm.bias True\n","bert.encoder.layer.0.intermediate.dense.weight True\n","bert.encoder.layer.0.intermediate.dense.bias True\n","bert.encoder.layer.0.output.dense.weight True\n","bert.encoder.layer.0.output.dense.bias True\n","bert.encoder.layer.0.output.LayerNorm.weight True\n","bert.encoder.layer.0.output.LayerNorm.bias True\n","bert.encoder.layer.1.attention.self.query.weight True\n","bert.encoder.layer.1.attention.self.query.bias True\n","bert.encoder.layer.1.attention.self.key.weight True\n","bert.encoder.layer.1.attention.self.key.bias True\n","bert.encoder.layer.1.attention.self.value.weight True\n","bert.encoder.layer.1.attention.self.value.bias True\n","bert.encoder.layer.1.attention.output.dense.weight True\n","bert.encoder.layer.1.attention.output.dense.bias True\n","bert.encoder.layer.1.attention.output.LayerNorm.weight True\n","bert.encoder.layer.1.attention.output.LayerNorm.bias True\n","bert.encoder.layer.1.intermediate.dense.weight True\n","bert.encoder.layer.1.intermediate.dense.bias True\n","bert.encoder.layer.1.output.dense.weight True\n","bert.encoder.layer.1.output.dense.bias True\n","bert.encoder.layer.1.output.LayerNorm.weight True\n","bert.encoder.layer.1.output.LayerNorm.bias True\n","bert.encoder.layer.2.attention.self.query.weight True\n","bert.encoder.layer.2.attention.self.query.bias True\n","bert.encoder.layer.2.attention.self.key.weight True\n","bert.encoder.layer.2.attention.self.key.bias True\n","bert.encoder.layer.2.attention.self.value.weight True\n","bert.encoder.layer.2.attention.self.value.bias True\n","bert.encoder.layer.2.attention.output.dense.weight True\n","bert.encoder.layer.2.attention.output.dense.bias True\n","bert.encoder.layer.2.attention.output.LayerNorm.weight True\n","bert.encoder.layer.2.attention.output.LayerNorm.bias True\n","bert.encoder.layer.2.intermediate.dense.weight True\n","bert.encoder.layer.2.intermediate.dense.bias True\n","bert.encoder.layer.2.output.dense.weight True\n","bert.encoder.layer.2.output.dense.bias True\n","bert.encoder.layer.2.output.LayerNorm.weight True\n","bert.encoder.layer.2.output.LayerNorm.bias True\n","bert.encoder.layer.3.attention.self.query.weight True\n","bert.encoder.layer.3.attention.self.query.bias True\n","bert.encoder.layer.3.attention.self.key.weight True\n","bert.encoder.layer.3.attention.self.key.bias True\n","bert.encoder.layer.3.attention.self.value.weight True\n","bert.encoder.layer.3.attention.self.value.bias True\n","bert.encoder.layer.3.attention.output.dense.weight True\n","bert.encoder.layer.3.attention.output.dense.bias True\n","bert.encoder.layer.3.attention.output.LayerNorm.weight True\n","bert.encoder.layer.3.attention.output.LayerNorm.bias True\n","bert.encoder.layer.3.intermediate.dense.weight True\n","bert.encoder.layer.3.intermediate.dense.bias True\n","bert.encoder.layer.3.output.dense.weight True\n","bert.encoder.layer.3.output.dense.bias True\n","bert.encoder.layer.3.output.LayerNorm.weight True\n","bert.encoder.layer.3.output.LayerNorm.bias True\n","bert.encoder.layer.4.attention.self.query.weight True\n","bert.encoder.layer.4.attention.self.query.bias True\n","bert.encoder.layer.4.attention.self.key.weight True\n","bert.encoder.layer.4.attention.self.key.bias True\n","bert.encoder.layer.4.attention.self.value.weight True\n","bert.encoder.layer.4.attention.self.value.bias True\n","bert.encoder.layer.4.attention.output.dense.weight True\n","bert.encoder.layer.4.attention.output.dense.bias True\n","bert.encoder.layer.4.attention.output.LayerNorm.weight True\n","bert.encoder.layer.4.attention.output.LayerNorm.bias True\n","bert.encoder.layer.4.intermediate.dense.weight True\n","bert.encoder.layer.4.intermediate.dense.bias True\n","bert.encoder.layer.4.output.dense.weight True\n","bert.encoder.layer.4.output.dense.bias True\n","bert.encoder.layer.4.output.LayerNorm.weight True\n","bert.encoder.layer.4.output.LayerNorm.bias True\n","bert.encoder.layer.5.attention.self.query.weight True\n","bert.encoder.layer.5.attention.self.query.bias True\n","bert.encoder.layer.5.attention.self.key.weight True\n","bert.encoder.layer.5.attention.self.key.bias True\n","bert.encoder.layer.5.attention.self.value.weight True\n","bert.encoder.layer.5.attention.self.value.bias True\n","bert.encoder.layer.5.attention.output.dense.weight True\n","bert.encoder.layer.5.attention.output.dense.bias True\n","bert.encoder.layer.5.attention.output.LayerNorm.weight True\n","bert.encoder.layer.5.attention.output.LayerNorm.bias True\n","bert.encoder.layer.5.intermediate.dense.weight True\n","bert.encoder.layer.5.intermediate.dense.bias True\n","bert.encoder.layer.5.output.dense.weight True\n","bert.encoder.layer.5.output.dense.bias True\n","bert.encoder.layer.5.output.LayerNorm.weight True\n","bert.encoder.layer.5.output.LayerNorm.bias True\n","bert.encoder.layer.6.attention.self.query.weight True\n","bert.encoder.layer.6.attention.self.query.bias True\n","bert.encoder.layer.6.attention.self.key.weight True\n","bert.encoder.layer.6.attention.self.key.bias True\n","bert.encoder.layer.6.attention.self.value.weight True\n","bert.encoder.layer.6.attention.self.value.bias True\n","bert.encoder.layer.6.attention.output.dense.weight True\n","bert.encoder.layer.6.attention.output.dense.bias True\n","bert.encoder.layer.6.attention.output.LayerNorm.weight True\n","bert.encoder.layer.6.attention.output.LayerNorm.bias True\n","bert.encoder.layer.6.intermediate.dense.weight True\n","bert.encoder.layer.6.intermediate.dense.bias True\n","bert.encoder.layer.6.output.dense.weight True\n","bert.encoder.layer.6.output.dense.bias True\n","bert.encoder.layer.6.output.LayerNorm.weight True\n","bert.encoder.layer.6.output.LayerNorm.bias True\n","bert.encoder.layer.7.attention.self.query.weight True\n","bert.encoder.layer.7.attention.self.query.bias True\n","bert.encoder.layer.7.attention.self.key.weight True\n","bert.encoder.layer.7.attention.self.key.bias True\n","bert.encoder.layer.7.attention.self.value.weight True\n","bert.encoder.layer.7.attention.self.value.bias True\n","bert.encoder.layer.7.attention.output.dense.weight True\n","bert.encoder.layer.7.attention.output.dense.bias True\n","bert.encoder.layer.7.attention.output.LayerNorm.weight True\n","bert.encoder.layer.7.attention.output.LayerNorm.bias True\n","bert.encoder.layer.7.intermediate.dense.weight True\n","bert.encoder.layer.7.intermediate.dense.bias True\n","bert.encoder.layer.7.output.dense.weight True\n","bert.encoder.layer.7.output.dense.bias True\n","bert.encoder.layer.7.output.LayerNorm.weight True\n","bert.encoder.layer.7.output.LayerNorm.bias True\n","bert.encoder.layer.8.attention.self.query.weight True\n","bert.encoder.layer.8.attention.self.query.bias True\n","bert.encoder.layer.8.attention.self.key.weight True\n","bert.encoder.layer.8.attention.self.key.bias True\n","bert.encoder.layer.8.attention.self.value.weight True\n","bert.encoder.layer.8.attention.self.value.bias True\n","bert.encoder.layer.8.attention.output.dense.weight True\n","bert.encoder.layer.8.attention.output.dense.bias True\n","bert.encoder.layer.8.attention.output.LayerNorm.weight True\n","bert.encoder.layer.8.attention.output.LayerNorm.bias True\n","bert.encoder.layer.8.intermediate.dense.weight True\n","bert.encoder.layer.8.intermediate.dense.bias True\n","bert.encoder.layer.8.output.dense.weight True\n","bert.encoder.layer.8.output.dense.bias True\n","bert.encoder.layer.8.output.LayerNorm.weight True\n","bert.encoder.layer.8.output.LayerNorm.bias True\n","bert.encoder.layer.9.attention.self.query.weight True\n","bert.encoder.layer.9.attention.self.query.bias True\n","bert.encoder.layer.9.attention.self.key.weight True\n","bert.encoder.layer.9.attention.self.key.bias True\n","bert.encoder.layer.9.attention.self.value.weight True\n","bert.encoder.layer.9.attention.self.value.bias True\n","bert.encoder.layer.9.attention.output.dense.weight True\n","bert.encoder.layer.9.attention.output.dense.bias True\n","bert.encoder.layer.9.attention.output.LayerNorm.weight True\n","bert.encoder.layer.9.attention.output.LayerNorm.bias True\n","bert.encoder.layer.9.intermediate.dense.weight True\n","bert.encoder.layer.9.intermediate.dense.bias True\n","bert.encoder.layer.9.output.dense.weight True\n","bert.encoder.layer.9.output.dense.bias True\n","bert.encoder.layer.9.output.LayerNorm.weight True\n","bert.encoder.layer.9.output.LayerNorm.bias True\n","bert.encoder.layer.10.attention.self.query.weight True\n","bert.encoder.layer.10.attention.self.query.bias True\n","bert.encoder.layer.10.attention.self.key.weight True\n","bert.encoder.layer.10.attention.self.key.bias True\n","bert.encoder.layer.10.attention.self.value.weight True\n","bert.encoder.layer.10.attention.self.value.bias True\n","bert.encoder.layer.10.attention.output.dense.weight True\n","bert.encoder.layer.10.attention.output.dense.bias True\n","bert.encoder.layer.10.attention.output.LayerNorm.weight True\n","bert.encoder.layer.10.attention.output.LayerNorm.bias True\n","bert.encoder.layer.10.intermediate.dense.weight True\n","bert.encoder.layer.10.intermediate.dense.bias True\n","bert.encoder.layer.10.output.dense.weight True\n","bert.encoder.layer.10.output.dense.bias True\n","bert.encoder.layer.10.output.LayerNorm.weight True\n","bert.encoder.layer.10.output.LayerNorm.bias True\n","bert.encoder.layer.11.attention.self.query.weight True\n","bert.encoder.layer.11.attention.self.query.bias True\n","bert.encoder.layer.11.attention.self.key.weight True\n","bert.encoder.layer.11.attention.self.key.bias True\n","bert.encoder.layer.11.attention.self.value.weight True\n","bert.encoder.layer.11.attention.self.value.bias True\n","bert.encoder.layer.11.attention.output.dense.weight True\n","bert.encoder.layer.11.attention.output.dense.bias True\n","bert.encoder.layer.11.attention.output.LayerNorm.weight True\n","bert.encoder.layer.11.attention.output.LayerNorm.bias True\n","bert.encoder.layer.11.intermediate.dense.weight True\n","bert.encoder.layer.11.intermediate.dense.bias True\n","bert.encoder.layer.11.output.dense.weight True\n","bert.encoder.layer.11.output.dense.bias True\n","bert.encoder.layer.11.output.LayerNorm.weight True\n","bert.encoder.layer.11.output.LayerNorm.bias True\n","bert.pooler.dense.weight True\n","bert.pooler.dense.bias True\n","classifier.weight True\n","classifier.bias True\n"]}]},{"cell_type":"markdown","source":["# Freezing and Unfreezing Model Parameters\n","\n","This code is used to selectively freeze and unfreeze the parameters of a pre-trained BERT model during fine-tuning.\n","\n","\n"],"metadata":{"id":"bVD0DQWClLBU"}},{"cell_type":"code","source":["# freeze base model parameters\n","for name, param in model.base_model.named_parameters():\n","    param.requires_grad = False\n","\n","# unfreeze base model pooling layers\n","for name, param in model.base_model.named_parameters():\n","    if \"pooler\" in name:\n","        param.requires_grad = True"],"metadata":{"id":"qHIPgu3kjJop","executionInfo":{"status":"ok","timestamp":1742933463643,"user_tz":-330,"elapsed":22,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Explanation of Printing Model Layers and Trainable Status\n","\n","This code prints the names of all the parameters in the model along with their `requires_grad` status to verify which layers are trainable and which are frozen.\n"],"metadata":{"id":"z7Wl9poClVz2"}},{"cell_type":"code","source":["\n","# print layers\n","for name, param in model.named_parameters():\n","   print(name, param.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmiefxfyjNrK","executionInfo":{"status":"ok","timestamp":1742933463674,"user_tz":-330,"elapsed":28,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"1665b17c-3b68-4de4-995f-b212a6cc4901"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["bert.embeddings.word_embeddings.weight False\n","bert.embeddings.position_embeddings.weight False\n","bert.embeddings.token_type_embeddings.weight False\n","bert.embeddings.LayerNorm.weight False\n","bert.embeddings.LayerNorm.bias False\n","bert.encoder.layer.0.attention.self.query.weight False\n","bert.encoder.layer.0.attention.self.query.bias False\n","bert.encoder.layer.0.attention.self.key.weight False\n","bert.encoder.layer.0.attention.self.key.bias False\n","bert.encoder.layer.0.attention.self.value.weight False\n","bert.encoder.layer.0.attention.self.value.bias False\n","bert.encoder.layer.0.attention.output.dense.weight False\n","bert.encoder.layer.0.attention.output.dense.bias False\n","bert.encoder.layer.0.attention.output.LayerNorm.weight False\n","bert.encoder.layer.0.attention.output.LayerNorm.bias False\n","bert.encoder.layer.0.intermediate.dense.weight False\n","bert.encoder.layer.0.intermediate.dense.bias False\n","bert.encoder.layer.0.output.dense.weight False\n","bert.encoder.layer.0.output.dense.bias False\n","bert.encoder.layer.0.output.LayerNorm.weight False\n","bert.encoder.layer.0.output.LayerNorm.bias False\n","bert.encoder.layer.1.attention.self.query.weight False\n","bert.encoder.layer.1.attention.self.query.bias False\n","bert.encoder.layer.1.attention.self.key.weight False\n","bert.encoder.layer.1.attention.self.key.bias False\n","bert.encoder.layer.1.attention.self.value.weight False\n","bert.encoder.layer.1.attention.self.value.bias False\n","bert.encoder.layer.1.attention.output.dense.weight False\n","bert.encoder.layer.1.attention.output.dense.bias False\n","bert.encoder.layer.1.attention.output.LayerNorm.weight False\n","bert.encoder.layer.1.attention.output.LayerNorm.bias False\n","bert.encoder.layer.1.intermediate.dense.weight False\n","bert.encoder.layer.1.intermediate.dense.bias False\n","bert.encoder.layer.1.output.dense.weight False\n","bert.encoder.layer.1.output.dense.bias False\n","bert.encoder.layer.1.output.LayerNorm.weight False\n","bert.encoder.layer.1.output.LayerNorm.bias False\n","bert.encoder.layer.2.attention.self.query.weight False\n","bert.encoder.layer.2.attention.self.query.bias False\n","bert.encoder.layer.2.attention.self.key.weight False\n","bert.encoder.layer.2.attention.self.key.bias False\n","bert.encoder.layer.2.attention.self.value.weight False\n","bert.encoder.layer.2.attention.self.value.bias False\n","bert.encoder.layer.2.attention.output.dense.weight False\n","bert.encoder.layer.2.attention.output.dense.bias False\n","bert.encoder.layer.2.attention.output.LayerNorm.weight False\n","bert.encoder.layer.2.attention.output.LayerNorm.bias False\n","bert.encoder.layer.2.intermediate.dense.weight False\n","bert.encoder.layer.2.intermediate.dense.bias False\n","bert.encoder.layer.2.output.dense.weight False\n","bert.encoder.layer.2.output.dense.bias False\n","bert.encoder.layer.2.output.LayerNorm.weight False\n","bert.encoder.layer.2.output.LayerNorm.bias False\n","bert.encoder.layer.3.attention.self.query.weight False\n","bert.encoder.layer.3.attention.self.query.bias False\n","bert.encoder.layer.3.attention.self.key.weight False\n","bert.encoder.layer.3.attention.self.key.bias False\n","bert.encoder.layer.3.attention.self.value.weight False\n","bert.encoder.layer.3.attention.self.value.bias False\n","bert.encoder.layer.3.attention.output.dense.weight False\n","bert.encoder.layer.3.attention.output.dense.bias False\n","bert.encoder.layer.3.attention.output.LayerNorm.weight False\n","bert.encoder.layer.3.attention.output.LayerNorm.bias False\n","bert.encoder.layer.3.intermediate.dense.weight False\n","bert.encoder.layer.3.intermediate.dense.bias False\n","bert.encoder.layer.3.output.dense.weight False\n","bert.encoder.layer.3.output.dense.bias False\n","bert.encoder.layer.3.output.LayerNorm.weight False\n","bert.encoder.layer.3.output.LayerNorm.bias False\n","bert.encoder.layer.4.attention.self.query.weight False\n","bert.encoder.layer.4.attention.self.query.bias False\n","bert.encoder.layer.4.attention.self.key.weight False\n","bert.encoder.layer.4.attention.self.key.bias False\n","bert.encoder.layer.4.attention.self.value.weight False\n","bert.encoder.layer.4.attention.self.value.bias False\n","bert.encoder.layer.4.attention.output.dense.weight False\n","bert.encoder.layer.4.attention.output.dense.bias False\n","bert.encoder.layer.4.attention.output.LayerNorm.weight False\n","bert.encoder.layer.4.attention.output.LayerNorm.bias False\n","bert.encoder.layer.4.intermediate.dense.weight False\n","bert.encoder.layer.4.intermediate.dense.bias False\n","bert.encoder.layer.4.output.dense.weight False\n","bert.encoder.layer.4.output.dense.bias False\n","bert.encoder.layer.4.output.LayerNorm.weight False\n","bert.encoder.layer.4.output.LayerNorm.bias False\n","bert.encoder.layer.5.attention.self.query.weight False\n","bert.encoder.layer.5.attention.self.query.bias False\n","bert.encoder.layer.5.attention.self.key.weight False\n","bert.encoder.layer.5.attention.self.key.bias False\n","bert.encoder.layer.5.attention.self.value.weight False\n","bert.encoder.layer.5.attention.self.value.bias False\n","bert.encoder.layer.5.attention.output.dense.weight False\n","bert.encoder.layer.5.attention.output.dense.bias False\n","bert.encoder.layer.5.attention.output.LayerNorm.weight False\n","bert.encoder.layer.5.attention.output.LayerNorm.bias False\n","bert.encoder.layer.5.intermediate.dense.weight False\n","bert.encoder.layer.5.intermediate.dense.bias False\n","bert.encoder.layer.5.output.dense.weight False\n","bert.encoder.layer.5.output.dense.bias False\n","bert.encoder.layer.5.output.LayerNorm.weight False\n","bert.encoder.layer.5.output.LayerNorm.bias False\n","bert.encoder.layer.6.attention.self.query.weight False\n","bert.encoder.layer.6.attention.self.query.bias False\n","bert.encoder.layer.6.attention.self.key.weight False\n","bert.encoder.layer.6.attention.self.key.bias False\n","bert.encoder.layer.6.attention.self.value.weight False\n","bert.encoder.layer.6.attention.self.value.bias False\n","bert.encoder.layer.6.attention.output.dense.weight False\n","bert.encoder.layer.6.attention.output.dense.bias False\n","bert.encoder.layer.6.attention.output.LayerNorm.weight False\n","bert.encoder.layer.6.attention.output.LayerNorm.bias False\n","bert.encoder.layer.6.intermediate.dense.weight False\n","bert.encoder.layer.6.intermediate.dense.bias False\n","bert.encoder.layer.6.output.dense.weight False\n","bert.encoder.layer.6.output.dense.bias False\n","bert.encoder.layer.6.output.LayerNorm.weight False\n","bert.encoder.layer.6.output.LayerNorm.bias False\n","bert.encoder.layer.7.attention.self.query.weight False\n","bert.encoder.layer.7.attention.self.query.bias False\n","bert.encoder.layer.7.attention.self.key.weight False\n","bert.encoder.layer.7.attention.self.key.bias False\n","bert.encoder.layer.7.attention.self.value.weight False\n","bert.encoder.layer.7.attention.self.value.bias False\n","bert.encoder.layer.7.attention.output.dense.weight False\n","bert.encoder.layer.7.attention.output.dense.bias False\n","bert.encoder.layer.7.attention.output.LayerNorm.weight False\n","bert.encoder.layer.7.attention.output.LayerNorm.bias False\n","bert.encoder.layer.7.intermediate.dense.weight False\n","bert.encoder.layer.7.intermediate.dense.bias False\n","bert.encoder.layer.7.output.dense.weight False\n","bert.encoder.layer.7.output.dense.bias False\n","bert.encoder.layer.7.output.LayerNorm.weight False\n","bert.encoder.layer.7.output.LayerNorm.bias False\n","bert.encoder.layer.8.attention.self.query.weight False\n","bert.encoder.layer.8.attention.self.query.bias False\n","bert.encoder.layer.8.attention.self.key.weight False\n","bert.encoder.layer.8.attention.self.key.bias False\n","bert.encoder.layer.8.attention.self.value.weight False\n","bert.encoder.layer.8.attention.self.value.bias False\n","bert.encoder.layer.8.attention.output.dense.weight False\n","bert.encoder.layer.8.attention.output.dense.bias False\n","bert.encoder.layer.8.attention.output.LayerNorm.weight False\n","bert.encoder.layer.8.attention.output.LayerNorm.bias False\n","bert.encoder.layer.8.intermediate.dense.weight False\n","bert.encoder.layer.8.intermediate.dense.bias False\n","bert.encoder.layer.8.output.dense.weight False\n","bert.encoder.layer.8.output.dense.bias False\n","bert.encoder.layer.8.output.LayerNorm.weight False\n","bert.encoder.layer.8.output.LayerNorm.bias False\n","bert.encoder.layer.9.attention.self.query.weight False\n","bert.encoder.layer.9.attention.self.query.bias False\n","bert.encoder.layer.9.attention.self.key.weight False\n","bert.encoder.layer.9.attention.self.key.bias False\n","bert.encoder.layer.9.attention.self.value.weight False\n","bert.encoder.layer.9.attention.self.value.bias False\n","bert.encoder.layer.9.attention.output.dense.weight False\n","bert.encoder.layer.9.attention.output.dense.bias False\n","bert.encoder.layer.9.attention.output.LayerNorm.weight False\n","bert.encoder.layer.9.attention.output.LayerNorm.bias False\n","bert.encoder.layer.9.intermediate.dense.weight False\n","bert.encoder.layer.9.intermediate.dense.bias False\n","bert.encoder.layer.9.output.dense.weight False\n","bert.encoder.layer.9.output.dense.bias False\n","bert.encoder.layer.9.output.LayerNorm.weight False\n","bert.encoder.layer.9.output.LayerNorm.bias False\n","bert.encoder.layer.10.attention.self.query.weight False\n","bert.encoder.layer.10.attention.self.query.bias False\n","bert.encoder.layer.10.attention.self.key.weight False\n","bert.encoder.layer.10.attention.self.key.bias False\n","bert.encoder.layer.10.attention.self.value.weight False\n","bert.encoder.layer.10.attention.self.value.bias False\n","bert.encoder.layer.10.attention.output.dense.weight False\n","bert.encoder.layer.10.attention.output.dense.bias False\n","bert.encoder.layer.10.attention.output.LayerNorm.weight False\n","bert.encoder.layer.10.attention.output.LayerNorm.bias False\n","bert.encoder.layer.10.intermediate.dense.weight False\n","bert.encoder.layer.10.intermediate.dense.bias False\n","bert.encoder.layer.10.output.dense.weight False\n","bert.encoder.layer.10.output.dense.bias False\n","bert.encoder.layer.10.output.LayerNorm.weight False\n","bert.encoder.layer.10.output.LayerNorm.bias False\n","bert.encoder.layer.11.attention.self.query.weight False\n","bert.encoder.layer.11.attention.self.query.bias False\n","bert.encoder.layer.11.attention.self.key.weight False\n","bert.encoder.layer.11.attention.self.key.bias False\n","bert.encoder.layer.11.attention.self.value.weight False\n","bert.encoder.layer.11.attention.self.value.bias False\n","bert.encoder.layer.11.attention.output.dense.weight False\n","bert.encoder.layer.11.attention.output.dense.bias False\n","bert.encoder.layer.11.attention.output.LayerNorm.weight False\n","bert.encoder.layer.11.attention.output.LayerNorm.bias False\n","bert.encoder.layer.11.intermediate.dense.weight False\n","bert.encoder.layer.11.intermediate.dense.bias False\n","bert.encoder.layer.11.output.dense.weight False\n","bert.encoder.layer.11.output.dense.bias False\n","bert.encoder.layer.11.output.LayerNorm.weight False\n","bert.encoder.layer.11.output.LayerNorm.bias False\n","bert.pooler.dense.weight True\n","bert.pooler.dense.bias True\n","classifier.weight True\n","classifier.bias True\n"]}]},{"cell_type":"markdown","source":["# Text Preprocessing Function\n","\n","This cell defines a simple preprocessing function using the tokenizer to prepare text data for input into the model.\n","\n"],"metadata":{"id":"b7OdwcKQmFVR"}},{"cell_type":"code","source":["# define text preprocessing\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)"],"metadata":{"id":"FSMkoFKfjPYU","executionInfo":{"status":"ok","timestamp":1742933463680,"user_tz":-330,"elapsed":4,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing All Datasets\n","\n","This cell applies the previously defined `preprocess_function` to tokenize all the datasets using the `map` function from the Hugging Face Datasets library.\n","\n"],"metadata":{"id":"exAtwuVlmONJ"}},{"cell_type":"code","source":["# tokenize all datasetse\n","tokenized_data = dataset_dict.map(preprocess_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f9812e9f4d964c1eb33365ade86cb799","36648f68db1542bf890e33f0c48e9497","aa5fb1c7ff2f4779991da0a2336dd0e8","91c78c8480b742eabbd7c093a1bd4347","db4137a409ca487f97018b6b98f3fbe8","f97cc38e4970428d9cbcc604c9b5d449","828dd5d1fde646e3a100c3bd38e68514","8da87bc0b4c8415b84e8e7b84d063ae1","852ccb25723b4f0ca1a1e1080306d04b","2f865a45c28f480c8d4dbb21c980fac0","9858bbe03c424c3cab3be3f746451276"]},"id":"cKab6rnUjSkI","executionInfo":{"status":"ok","timestamp":1742933463733,"user_tz":-330,"elapsed":52,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"3a16fdb9-93ed-4816-a424-8c0d8b6ab2ec"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/450 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9812e9f4d964c1eb33365ade86cb799"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Creating a Data Collator\n","\n","This cell creates a **Data Collator** using the `DataCollatorWithPadding` class from Hugging Face Transformers.\n","\n"],"metadata":{"id":"Y7LA0kYpmaBY"}},{"cell_type":"code","source":["# create data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"Ex0Tha2yjUSS","executionInfo":{"status":"ok","timestamp":1742933463747,"user_tz":-330,"elapsed":13,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Loading Metrics and Computing Evaluation Metrics\n","\n","This cell is responsible for evaluating the model's performance using accuracy and AUC (Area Under the Curve) metrics.\n","\n"],"metadata":{"id":"4LfPPbRgm14h"}},{"cell_type":"code","source":["# load metrics\n","accuracy = evaluate.load(\"accuracy\")\n","auc_score = evaluate.load(\"roc_auc\")\n","\n","def compute_metrics(eval_pred):\n","    # get predictions\n","    predictions, labels = eval_pred\n","\n","    # apply softmax to get probabilities\n","    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n","    # use probabilities of the positive class for ROC AUC\n","    positive_class_probs = probabilities[:, 1]\n","    # compute auc\n","    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n","\n","    # predict most probable class\n","    predicted_classes = np.argmax(predictions, axis=1)\n","    # compute accuracy\n","    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n","\n","    return {\"Accuracy\": acc, \"AUC\": auc}"],"metadata":{"id":"h7r0ebJSjW0Z","executionInfo":{"status":"ok","timestamp":1742933464636,"user_tz":-330,"elapsed":890,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Explanation of Hyperparameters and TrainingArguments\n","\n","This cell defines the hyperparameters for the model training process and sets up the training configuration using the `TrainingArguments` class from the Hugging Face Transformers library.\n"],"metadata":{"id":"P9OAl_aFnBqP"}},{"cell_type":"code","source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 8\n","num_epochs = 10\n","\n","training_args = TrainingArguments(\n","    output_dir=\"bert-phishing-classifier_teacher\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    logging_strategy=\"epoch\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"],"metadata":{"id":"T3xUGW4VjY-a","executionInfo":{"status":"ok","timestamp":1742933464753,"user_tz":-330,"elapsed":115,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Emissions Tracking and Model Training with CodeCarbon\n","\n","This cell performs two key functions:  \n","1. **Training the Model**: It uses the Hugging Face `Trainer` class to fine-tune the BERT model on the phishing site classification dataset.  \n","2. **Tracking Emissions**: The `EmissionsTracker` from CodeCarbon monitors and reports the carbon footprint generated during the training process.\n"],"metadata":{"id":"WOjTrD66nMw0"}},{"cell_type":"code","source":["from codecarbon import EmissionsTracker\n","from transformers import Trainer\n","\n","# Assuming trainer is already set up\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model with emissions tracking\n","with EmissionsTracker(output_dir='/content/output', output_file='emissions.csv', allow_multiple_runs=True) as tracker:\n","    trainer.train()\n","\n","# Get total emissions directly from the tracker\n","total_emissions = tracker.final_emissions\n","print(f\"Total emissions: {total_emissions} kgCO2eq\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3cR5x1WUjcEZ","outputId":"30ffe185-e23c-480a-b74a-170c6e5cb9c4","executionInfo":{"status":"ok","timestamp":1742933626997,"user_tz":-330,"elapsed":162231,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-0ee7a160f0f8>:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","[codecarbon ERROR @ 20:11:08] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n","[codecarbon WARNING @ 20:11:08] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon INFO @ 20:11:08] [setup] RAM Tracking...\n","[codecarbon INFO @ 20:11:08] [setup] CPU Tracking...\n","[codecarbon WARNING @ 20:11:08] No CPU tracking mode found. Falling back on CPU constant mode. \n"," Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n","\n","[codecarbon WARNING @ 20:11:09] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n","[codecarbon INFO @ 20:11:09] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n","[codecarbon INFO @ 20:11:09] [setup] GPU Tracking...\n","[codecarbon INFO @ 20:11:09] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 20:11:09] >>> Tracker's metadata:\n","[codecarbon INFO @ 20:11:09]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n","[codecarbon INFO @ 20:11:09]   Python version: 3.11.11\n","[codecarbon INFO @ 20:11:09]   CodeCarbon version: 2.8.3\n","[codecarbon INFO @ 20:11:09]   Available RAM : 12.675 GB\n","[codecarbon INFO @ 20:11:09]   CPU count: 2\n","[codecarbon INFO @ 20:11:09]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n","[codecarbon INFO @ 20:11:09]   GPU count: 1\n","[codecarbon INFO @ 20:11:09]   GPU model: 1 x Tesla T4\n","[codecarbon INFO @ 20:11:10] Saving emissions data to file /content/output/emissions.csv\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhatteryash21\u001b[0m (\u001b[33mbhatteryash21-gokaraju-rangaraju-institute-of-engineerin\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250325_201112-nm0czj1h</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface/runs/nm0czj1h' target=\"_blank\">bert-phishing-classifier_teacher</a></strong> to <a href='https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface' target=\"_blank\">https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface/runs/nm0czj1h' target=\"_blank\">https://wandb.ai/bhatteryash21-gokaraju-rangaraju-institute-of-engineerin/huggingface/runs/nm0czj1h</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[codecarbon WARNING @ 20:11:14] Another instance of codecarbon is already running. Exiting.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2630/2630 02:31, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.507600</td>\n","      <td>0.390735</td>\n","      <td>0.804000</td>\n","      <td>0.912000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.409000</td>\n","      <td>0.341644</td>\n","      <td>0.833000</td>\n","      <td>0.930000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.356600</td>\n","      <td>0.314357</td>\n","      <td>0.851000</td>\n","      <td>0.939000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.357400</td>\n","      <td>0.354922</td>\n","      <td>0.849000</td>\n","      <td>0.946000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.350400</td>\n","      <td>0.335314</td>\n","      <td>0.860000</td>\n","      <td>0.948000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.349300</td>\n","      <td>0.289885</td>\n","      <td>0.867000</td>\n","      <td>0.950000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.334300</td>\n","      <td>0.288745</td>\n","      <td>0.876000</td>\n","      <td>0.950000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.312200</td>\n","      <td>0.288694</td>\n","      <td>0.869000</td>\n","      <td>0.950000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.312000</td>\n","      <td>0.285123</td>\n","      <td>0.867000</td>\n","      <td>0.951000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.313300</td>\n","      <td>0.290017</td>\n","      <td>0.867000</td>\n","      <td>0.951000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[codecarbon INFO @ 20:11:25] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:11:25] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:11:25] Energy consumed for all GPUs : 0.000219 kWh. Total GPU Power : 52.48887440242671 W\n","[codecarbon INFO @ 20:11:25] 0.000416 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:11:40] Energy consumed for RAM : 0.000040 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:11:40] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:11:40] Energy consumed for all GPUs : 0.000470 kWh. Total GPU Power : 60.32000823669473 W\n","[codecarbon INFO @ 20:11:40] 0.000864 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:11:55] Energy consumed for RAM : 0.000059 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:11:55] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:11:55] Energy consumed for all GPUs : 0.000745 kWh. Total GPU Power : 65.85776197920502 W\n","[codecarbon INFO @ 20:11:55] 0.001335 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:12:10] Energy consumed for RAM : 0.000079 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:12:10] Energy consumed for all CPUs : 0.000708 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:12:10] Energy consumed for all GPUs : 0.001016 kWh. Total GPU Power : 65.29079854286415 W\n","[codecarbon INFO @ 20:12:10] 0.001804 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:12:25] Energy consumed for RAM : 0.000099 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:12:25] Energy consumed for all CPUs : 0.000885 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:12:25] Energy consumed for all GPUs : 0.001286 kWh. Total GPU Power : 64.71692202496615 W\n","[codecarbon INFO @ 20:12:25] 0.002270 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:12:40] Energy consumed for RAM : 0.000119 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:12:40] Energy consumed for all CPUs : 0.001062 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:12:40] Energy consumed for all GPUs : 0.001558 kWh. Total GPU Power : 65.34001267715765 W\n","[codecarbon INFO @ 20:12:40] 0.002739 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:12:55] Energy consumed for RAM : 0.000139 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:12:55] Energy consumed for all CPUs : 0.001239 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:12:55] Energy consumed for all GPUs : 0.001828 kWh. Total GPU Power : 64.6374139377861 W\n","[codecarbon INFO @ 20:12:55] 0.003206 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:13:10] Energy consumed for RAM : 0.000158 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:13:10] Energy consumed for all CPUs : 0.001416 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:13:10] Energy consumed for all GPUs : 0.002102 kWh. Total GPU Power : 65.95843551966851 W\n","[codecarbon INFO @ 20:13:10] 0.003677 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:13:10] 0.010695 g.CO2eq/s mean an estimation of 337.2921641570862 kg.CO2eq/year\n","[codecarbon INFO @ 20:13:25] Energy consumed for RAM : 0.000178 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:13:25] Energy consumed for all CPUs : 0.001593 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:13:25] Energy consumed for all GPUs : 0.002366 kWh. Total GPU Power : 63.34372413880253 W\n","[codecarbon INFO @ 20:13:25] 0.004138 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 20:13:40] Energy consumed for RAM : 0.000198 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:13:40] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:13:40] Energy consumed for all GPUs : 0.002610 kWh. Total GPU Power : 58.432006246821686 W\n","[codecarbon INFO @ 20:13:40] 0.004578 kWh of electricity used since the beginning.\n","[codecarbon WARNING @ 20:13:47] Another instance of codecarbon is already running. Exiting.\n","[codecarbon INFO @ 20:13:47] Energy consumed for RAM : 0.000207 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 20:13:47] Energy consumed for all CPUs : 0.001852 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 20:13:47] Energy consumed for all GPUs : 0.002722 kWh. Total GPU Power : 59.080768400395186 W\n","[codecarbon INFO @ 20:13:47] 0.004781 kWh of electricity used since the beginning.\n"]},{"output_type":"stream","name":"stdout","text":["Total emissions: 0.0016696896840862562 kgCO2eq\n"]}]},{"cell_type":"markdown","source":["# Validation and Emissions Evaluation\n","\n","This cell performs the following tasks:  \n","1. **Model Validation**: Applies the trained BERT model to the validation dataset and evaluates its performance.  \n","2. **Metrics Calculation**: Computes accuracy and AUC using the `compute_metrics()` function.  \n","3. **Emissions Reporting**: Reads and prints the total carbon emissions generated during training using the data stored in the `emissions.csv` file.  \n"],"metadata":{"id":"QY0HctnmnUS4"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Apply model to validation dataset\n","predictions = trainer.predict(tokenized_data[\"validation\"])\n","\n","# Extract the logits and labels from the predictions object\n","logits = predictions.predictions\n","labels = predictions.label_ids\n","\n","# Use your compute_metrics function\n","metrics = compute_metrics((logits, labels))\n","\n","# Read the total emissions from the correct CSV file path\n","emissions_df = pd.read_csv('/content/output/emissions.csv')\n","total_emissions = emissions_df['emissions'].iloc[-1]\n","\n","# Print the validation metrics and total emissions together\n","print(\"Validation Metrics and Total Emissions:\")\n","print(f\"Metrics: {metrics}\")\n","print(f\"Total emissions: {total_emissions} kgCO2eq\")"],"metadata":{"id":"r2au-UXPjeJF","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1742933628902,"user_tz":-330,"elapsed":1873,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"ae97d3a6-b2a1-4f11-8bd1-813a119404e2"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation Metrics and Total Emissions:\n","Metrics: {'Accuracy': np.float64(0.893), 'AUC': np.float64(0.946)}\n","Total emissions: 0.0016696896840862 kgCO2eq\n"]}]}]}