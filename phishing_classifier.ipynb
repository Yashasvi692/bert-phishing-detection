{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["Xf_MuzeFj08T","9BqRK0YFkAUy","6B3cYzcjkTln"],"authorship_tag":"ABX9TyM7zDeQ0cX0T0V7dCCRT4jy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"738fa62babce4ef180835da37c4c2aa7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_745c414042594ea1896f927193077689","IPY_MODEL_0d10521c53e045b58ef8b5956159b6ac","IPY_MODEL_a40677382d494dbcb58325df2fd4665e"],"layout":"IPY_MODEL_d832f73454b84e4182ceff611c351ce9"}},"745c414042594ea1896f927193077689":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c1ad1df69ff40768951cf62e11c6cb5","placeholder":"​","style":"IPY_MODEL_5372ffd17a3444448529f74df8909b5e","value":"Map: 100%"}},"0d10521c53e045b58ef8b5956159b6ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47c1543027b34720bfeb4f8c8d5b6ced","max":450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc7701b5e3f9490098cf217b8009f2eb","value":450}},"a40677382d494dbcb58325df2fd4665e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d45df5d878604b88a74bff6cf6b1e357","placeholder":"​","style":"IPY_MODEL_ccfde8bace4244ab9e5fbc3de244e0a1","value":" 450/450 [00:00&lt;00:00, 6067.76 examples/s]"}},"d832f73454b84e4182ceff611c351ce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c1ad1df69ff40768951cf62e11c6cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5372ffd17a3444448529f74df8909b5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47c1543027b34720bfeb4f8c8d5b6ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7701b5e3f9490098cf217b8009f2eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d45df5d878604b88a74bff6cf6b1e357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccfde8bace4244ab9e5fbc3de244e0a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install Required Packages\n","\n","In order to run this notebook, you need to have several Python packages installed. The commands below will install the following packages:\n","\n","- **datasets:** Provides access to a wide range of datasets and tools to load and process them.\n","- **transformers:** Contains pre-trained models and tools for working with transformer-based architectures.\n","- **evaluate:** Offers utilities to compute evaluation metrics.\n","- **numpy:** A fundamental package for numerical computing in Python.\n","- **codecarbon:** Tracks energy consumption and emissions during model training.\n","\n"],"metadata":{"id":"Xf_MuzeFj08T"}},{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n","!pip install evaluate\n","!pip install numpy\n","!pip install codecarbon"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XziwJBzhi-p","executionInfo":{"status":"ok","timestamp":1742841638105,"user_tz":-330,"elapsed":29823,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"5f5b298b-3c72-46df-fc05-f53a879e953c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: codecarbon in /usr/local/lib/python3.11/dist-packages (2.8.3)\n","Requirement already satisfied: arrow in /usr/local/lib/python3.11/dist-packages (from codecarbon) (1.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n","Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.20.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.2.2)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n","Requirement already satisfied: questionary in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.1.0)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from codecarbon) (3.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.32.3)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n","Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.2)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.8.2)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.9.0.20241206)\n","Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n","Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n","Requirement already satisfied: yaspin in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (3.1.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2.0.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n","Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n","Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2025.1.31)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (4.12.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n","Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n","Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n"]}]},{"cell_type":"markdown","source":["# Import Necessary Libraries\n","\n","This cell imports all the required libraries for loading datasets, preprocessing, model training, evaluation, and energy consumption tracking. Here's what each import does:\n","\n","- **load_dataset (from datasets):**  \n","  - Loads datasets from Hugging Face's Datasets library. It supports a variety of datasets for natural language processing (NLP) tasks.\n","\n","- **AutoTokenizer, AutoModelForSequenceClassification (from transformers):**  \n","  - `AutoTokenizer` automatically loads the appropriate tokenizer for a given model.\n","  - `AutoModelForSequenceClassification` loads a pretrained transformer model designed for text classification.\n","\n","- **TrainingArguments, Trainer (from transformers):**  \n","  - `TrainingArguments` allows you to configure parameters like batch size, learning rate, and number of epochs.\n","  - `Trainer` handles the training and evaluation processes using the Hugging Face Transformers library.\n","\n","- **evaluate:**  \n","  - Provides tools for computing evaluation metrics such as accuracy, F1-score, or recall.\n","\n","- **numpy:**  \n","  - Used for efficient numerical operations and matrix manipulations.\n","\n","- **DataCollatorWithPadding (from transformers):**  \n","  - Automatically pads sequences to the maximum length in a batch, ensuring consistent input size for the model.\n","\n","- **EmissionsTracker (from codecarbon):**  \n","  - Tracks the carbon footprint of the training process, providing insights into energy consumption and CO₂ emissions.\n","\n","These imports are essential for loading data, building models, training with transformers, evaluating results, and tracking environmental impact.\n","\n"],"metadata":{"id":"9BqRK0YFkAUy"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import evaluate\n","import numpy as np\n","from transformers import DataCollatorWithPadding\n","from codecarbon import EmissionsTracker"],"metadata":{"id":"xcNSVsGThe9_","executionInfo":{"status":"ok","timestamp":1742841673032,"user_tz":-330,"elapsed":27694,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Load the Dataset\n","\n","This line of code loads the **Phishing Site Classification** dataset using the Hugging Face Datasets library.\n","\n","***About the Dataset:***\n","\n","- **Dataset Name:** Phishing Site Classification\n","- **Source:** Hugging Face (Published by Shawhin)\n","- **Task:** Binary Classification — Classify URLs as either Safe or Not Safe.\n","- **Objective:** The dataset is specifically designed to train and evaluate machine learning models for detecting phishing sites."],"metadata":{"id":"2z7PqikDkLNN"}},{"cell_type":"code","source":["dataset_dict = load_dataset(\"shawhin/phishing-site-classification\")"],"metadata":{"id":"epud7_YRiHUQ","executionInfo":{"status":"ok","timestamp":1742841684268,"user_tz":-330,"elapsed":3389,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ffeffb41-e80b-4426-c91b-eff9a1f41886"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## View the Loaded Dataset\n","\n","\n"],"metadata":{"id":"6B3cYzcjkTln"}},{"cell_type":"code","source":["dataset_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwRSkD6_ipBz","executionInfo":{"status":"ok","timestamp":1742841697848,"user_tz":-330,"elapsed":57,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"38b2411f-2a3a-47a5-c3ad-3a5fd802e294"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 2100\n","    })\n","    validation: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 450\n","    })\n","    test: Dataset({\n","        features: ['text', 'labels'],\n","        num_rows: 450\n","    })\n","})"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["# Load Pretrained BERT Model and Tokenizer\n","\n","This section of code loads a pretrained **BERT (Bidirectional Encoder Representations from Transformers)** model and its tokenizer using Hugging Face's `transformers` library.\n"],"metadata":{"id":"PNu5ZoUNkgXP"}},{"cell_type":"code","source":["# Load model directly\n","model_path = \"google-bert/bert-base-uncased\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","id2label = {0: \"Safe\", 1: \"Not Safe\"}\n","label2id = {\"Safe\": 0, \"Not Safe\": 1}\n","model = AutoModelForSequenceClassification.from_pretrained(model_path,\n","                                                           num_labels=2,\n","                                                           id2label=id2label,\n","                                                           label2id=label2id,)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8E0BhimjCgZ","executionInfo":{"status":"ok","timestamp":1742841701634,"user_tz":-330,"elapsed":926,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"674a8e99-7e50-4cb5-ada6-0161ff4cd8dc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["# Print Model Layers and Trainable Parameters\n","\n","This code iterates through all the parameters of the model and prints their names along with their `requires_grad` status.\n","\n"],"metadata":{"id":"pjxsA9LTko0w"}},{"cell_type":"code","source":["# print layers\n","for name, param in model.named_parameters():\n","   print(name, param.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BurH3KQFjFaf","executionInfo":{"status":"ok","timestamp":1742841704149,"user_tz":-330,"elapsed":67,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"9f0f10e8-6f35-4344-c346-68646ae5fd6c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["bert.embeddings.word_embeddings.weight True\n","bert.embeddings.position_embeddings.weight True\n","bert.embeddings.token_type_embeddings.weight True\n","bert.embeddings.LayerNorm.weight True\n","bert.embeddings.LayerNorm.bias True\n","bert.encoder.layer.0.attention.self.query.weight True\n","bert.encoder.layer.0.attention.self.query.bias True\n","bert.encoder.layer.0.attention.self.key.weight True\n","bert.encoder.layer.0.attention.self.key.bias True\n","bert.encoder.layer.0.attention.self.value.weight True\n","bert.encoder.layer.0.attention.self.value.bias True\n","bert.encoder.layer.0.attention.output.dense.weight True\n","bert.encoder.layer.0.attention.output.dense.bias True\n","bert.encoder.layer.0.attention.output.LayerNorm.weight True\n","bert.encoder.layer.0.attention.output.LayerNorm.bias True\n","bert.encoder.layer.0.intermediate.dense.weight True\n","bert.encoder.layer.0.intermediate.dense.bias True\n","bert.encoder.layer.0.output.dense.weight True\n","bert.encoder.layer.0.output.dense.bias True\n","bert.encoder.layer.0.output.LayerNorm.weight True\n","bert.encoder.layer.0.output.LayerNorm.bias True\n","bert.encoder.layer.1.attention.self.query.weight True\n","bert.encoder.layer.1.attention.self.query.bias True\n","bert.encoder.layer.1.attention.self.key.weight True\n","bert.encoder.layer.1.attention.self.key.bias True\n","bert.encoder.layer.1.attention.self.value.weight True\n","bert.encoder.layer.1.attention.self.value.bias True\n","bert.encoder.layer.1.attention.output.dense.weight True\n","bert.encoder.layer.1.attention.output.dense.bias True\n","bert.encoder.layer.1.attention.output.LayerNorm.weight True\n","bert.encoder.layer.1.attention.output.LayerNorm.bias True\n","bert.encoder.layer.1.intermediate.dense.weight True\n","bert.encoder.layer.1.intermediate.dense.bias True\n","bert.encoder.layer.1.output.dense.weight True\n","bert.encoder.layer.1.output.dense.bias True\n","bert.encoder.layer.1.output.LayerNorm.weight True\n","bert.encoder.layer.1.output.LayerNorm.bias True\n","bert.encoder.layer.2.attention.self.query.weight True\n","bert.encoder.layer.2.attention.self.query.bias True\n","bert.encoder.layer.2.attention.self.key.weight True\n","bert.encoder.layer.2.attention.self.key.bias True\n","bert.encoder.layer.2.attention.self.value.weight True\n","bert.encoder.layer.2.attention.self.value.bias True\n","bert.encoder.layer.2.attention.output.dense.weight True\n","bert.encoder.layer.2.attention.output.dense.bias True\n","bert.encoder.layer.2.attention.output.LayerNorm.weight True\n","bert.encoder.layer.2.attention.output.LayerNorm.bias True\n","bert.encoder.layer.2.intermediate.dense.weight True\n","bert.encoder.layer.2.intermediate.dense.bias True\n","bert.encoder.layer.2.output.dense.weight True\n","bert.encoder.layer.2.output.dense.bias True\n","bert.encoder.layer.2.output.LayerNorm.weight True\n","bert.encoder.layer.2.output.LayerNorm.bias True\n","bert.encoder.layer.3.attention.self.query.weight True\n","bert.encoder.layer.3.attention.self.query.bias True\n","bert.encoder.layer.3.attention.self.key.weight True\n","bert.encoder.layer.3.attention.self.key.bias True\n","bert.encoder.layer.3.attention.self.value.weight True\n","bert.encoder.layer.3.attention.self.value.bias True\n","bert.encoder.layer.3.attention.output.dense.weight True\n","bert.encoder.layer.3.attention.output.dense.bias True\n","bert.encoder.layer.3.attention.output.LayerNorm.weight True\n","bert.encoder.layer.3.attention.output.LayerNorm.bias True\n","bert.encoder.layer.3.intermediate.dense.weight True\n","bert.encoder.layer.3.intermediate.dense.bias True\n","bert.encoder.layer.3.output.dense.weight True\n","bert.encoder.layer.3.output.dense.bias True\n","bert.encoder.layer.3.output.LayerNorm.weight True\n","bert.encoder.layer.3.output.LayerNorm.bias True\n","bert.encoder.layer.4.attention.self.query.weight True\n","bert.encoder.layer.4.attention.self.query.bias True\n","bert.encoder.layer.4.attention.self.key.weight True\n","bert.encoder.layer.4.attention.self.key.bias True\n","bert.encoder.layer.4.attention.self.value.weight True\n","bert.encoder.layer.4.attention.self.value.bias True\n","bert.encoder.layer.4.attention.output.dense.weight True\n","bert.encoder.layer.4.attention.output.dense.bias True\n","bert.encoder.layer.4.attention.output.LayerNorm.weight True\n","bert.encoder.layer.4.attention.output.LayerNorm.bias True\n","bert.encoder.layer.4.intermediate.dense.weight True\n","bert.encoder.layer.4.intermediate.dense.bias True\n","bert.encoder.layer.4.output.dense.weight True\n","bert.encoder.layer.4.output.dense.bias True\n","bert.encoder.layer.4.output.LayerNorm.weight True\n","bert.encoder.layer.4.output.LayerNorm.bias True\n","bert.encoder.layer.5.attention.self.query.weight True\n","bert.encoder.layer.5.attention.self.query.bias True\n","bert.encoder.layer.5.attention.self.key.weight True\n","bert.encoder.layer.5.attention.self.key.bias True\n","bert.encoder.layer.5.attention.self.value.weight True\n","bert.encoder.layer.5.attention.self.value.bias True\n","bert.encoder.layer.5.attention.output.dense.weight True\n","bert.encoder.layer.5.attention.output.dense.bias True\n","bert.encoder.layer.5.attention.output.LayerNorm.weight True\n","bert.encoder.layer.5.attention.output.LayerNorm.bias True\n","bert.encoder.layer.5.intermediate.dense.weight True\n","bert.encoder.layer.5.intermediate.dense.bias True\n","bert.encoder.layer.5.output.dense.weight True\n","bert.encoder.layer.5.output.dense.bias True\n","bert.encoder.layer.5.output.LayerNorm.weight True\n","bert.encoder.layer.5.output.LayerNorm.bias True\n","bert.encoder.layer.6.attention.self.query.weight True\n","bert.encoder.layer.6.attention.self.query.bias True\n","bert.encoder.layer.6.attention.self.key.weight True\n","bert.encoder.layer.6.attention.self.key.bias True\n","bert.encoder.layer.6.attention.self.value.weight True\n","bert.encoder.layer.6.attention.self.value.bias True\n","bert.encoder.layer.6.attention.output.dense.weight True\n","bert.encoder.layer.6.attention.output.dense.bias True\n","bert.encoder.layer.6.attention.output.LayerNorm.weight True\n","bert.encoder.layer.6.attention.output.LayerNorm.bias True\n","bert.encoder.layer.6.intermediate.dense.weight True\n","bert.encoder.layer.6.intermediate.dense.bias True\n","bert.encoder.layer.6.output.dense.weight True\n","bert.encoder.layer.6.output.dense.bias True\n","bert.encoder.layer.6.output.LayerNorm.weight True\n","bert.encoder.layer.6.output.LayerNorm.bias True\n","bert.encoder.layer.7.attention.self.query.weight True\n","bert.encoder.layer.7.attention.self.query.bias True\n","bert.encoder.layer.7.attention.self.key.weight True\n","bert.encoder.layer.7.attention.self.key.bias True\n","bert.encoder.layer.7.attention.self.value.weight True\n","bert.encoder.layer.7.attention.self.value.bias True\n","bert.encoder.layer.7.attention.output.dense.weight True\n","bert.encoder.layer.7.attention.output.dense.bias True\n","bert.encoder.layer.7.attention.output.LayerNorm.weight True\n","bert.encoder.layer.7.attention.output.LayerNorm.bias True\n","bert.encoder.layer.7.intermediate.dense.weight True\n","bert.encoder.layer.7.intermediate.dense.bias True\n","bert.encoder.layer.7.output.dense.weight True\n","bert.encoder.layer.7.output.dense.bias True\n","bert.encoder.layer.7.output.LayerNorm.weight True\n","bert.encoder.layer.7.output.LayerNorm.bias True\n","bert.encoder.layer.8.attention.self.query.weight True\n","bert.encoder.layer.8.attention.self.query.bias True\n","bert.encoder.layer.8.attention.self.key.weight True\n","bert.encoder.layer.8.attention.self.key.bias True\n","bert.encoder.layer.8.attention.self.value.weight True\n","bert.encoder.layer.8.attention.self.value.bias True\n","bert.encoder.layer.8.attention.output.dense.weight True\n","bert.encoder.layer.8.attention.output.dense.bias True\n","bert.encoder.layer.8.attention.output.LayerNorm.weight True\n","bert.encoder.layer.8.attention.output.LayerNorm.bias True\n","bert.encoder.layer.8.intermediate.dense.weight True\n","bert.encoder.layer.8.intermediate.dense.bias True\n","bert.encoder.layer.8.output.dense.weight True\n","bert.encoder.layer.8.output.dense.bias True\n","bert.encoder.layer.8.output.LayerNorm.weight True\n","bert.encoder.layer.8.output.LayerNorm.bias True\n","bert.encoder.layer.9.attention.self.query.weight True\n","bert.encoder.layer.9.attention.self.query.bias True\n","bert.encoder.layer.9.attention.self.key.weight True\n","bert.encoder.layer.9.attention.self.key.bias True\n","bert.encoder.layer.9.attention.self.value.weight True\n","bert.encoder.layer.9.attention.self.value.bias True\n","bert.encoder.layer.9.attention.output.dense.weight True\n","bert.encoder.layer.9.attention.output.dense.bias True\n","bert.encoder.layer.9.attention.output.LayerNorm.weight True\n","bert.encoder.layer.9.attention.output.LayerNorm.bias True\n","bert.encoder.layer.9.intermediate.dense.weight True\n","bert.encoder.layer.9.intermediate.dense.bias True\n","bert.encoder.layer.9.output.dense.weight True\n","bert.encoder.layer.9.output.dense.bias True\n","bert.encoder.layer.9.output.LayerNorm.weight True\n","bert.encoder.layer.9.output.LayerNorm.bias True\n","bert.encoder.layer.10.attention.self.query.weight True\n","bert.encoder.layer.10.attention.self.query.bias True\n","bert.encoder.layer.10.attention.self.key.weight True\n","bert.encoder.layer.10.attention.self.key.bias True\n","bert.encoder.layer.10.attention.self.value.weight True\n","bert.encoder.layer.10.attention.self.value.bias True\n","bert.encoder.layer.10.attention.output.dense.weight True\n","bert.encoder.layer.10.attention.output.dense.bias True\n","bert.encoder.layer.10.attention.output.LayerNorm.weight True\n","bert.encoder.layer.10.attention.output.LayerNorm.bias True\n","bert.encoder.layer.10.intermediate.dense.weight True\n","bert.encoder.layer.10.intermediate.dense.bias True\n","bert.encoder.layer.10.output.dense.weight True\n","bert.encoder.layer.10.output.dense.bias True\n","bert.encoder.layer.10.output.LayerNorm.weight True\n","bert.encoder.layer.10.output.LayerNorm.bias True\n","bert.encoder.layer.11.attention.self.query.weight True\n","bert.encoder.layer.11.attention.self.query.bias True\n","bert.encoder.layer.11.attention.self.key.weight True\n","bert.encoder.layer.11.attention.self.key.bias True\n","bert.encoder.layer.11.attention.self.value.weight True\n","bert.encoder.layer.11.attention.self.value.bias True\n","bert.encoder.layer.11.attention.output.dense.weight True\n","bert.encoder.layer.11.attention.output.dense.bias True\n","bert.encoder.layer.11.attention.output.LayerNorm.weight True\n","bert.encoder.layer.11.attention.output.LayerNorm.bias True\n","bert.encoder.layer.11.intermediate.dense.weight True\n","bert.encoder.layer.11.intermediate.dense.bias True\n","bert.encoder.layer.11.output.dense.weight True\n","bert.encoder.layer.11.output.dense.bias True\n","bert.encoder.layer.11.output.LayerNorm.weight True\n","bert.encoder.layer.11.output.LayerNorm.bias True\n","bert.pooler.dense.weight True\n","bert.pooler.dense.bias True\n","classifier.weight True\n","classifier.bias True\n"]}]},{"cell_type":"markdown","source":["# Freezing and Unfreezing Model Parameters\n","\n","This code is used to selectively freeze and unfreeze the parameters of a pre-trained BERT model during fine-tuning.\n","\n","\n"],"metadata":{"id":"bVD0DQWClLBU"}},{"cell_type":"code","source":["# freeze base model parameters\n","for name, param in model.base_model.named_parameters():\n","    param.requires_grad = False\n","\n","# unfreeze base model pooling layers\n","for name, param in model.base_model.named_parameters():\n","    if \"pooler\" in name:\n","        param.requires_grad = True"],"metadata":{"id":"qHIPgu3kjJop","executionInfo":{"status":"ok","timestamp":1742841705583,"user_tz":-330,"elapsed":9,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Explanation of Printing Model Layers and Trainable Status\n","\n","This code prints the names of all the parameters in the model along with their `requires_grad` status to verify which layers are trainable and which are frozen.\n"],"metadata":{"id":"z7Wl9poClVz2"}},{"cell_type":"code","source":["# print layers\n","for name, param in model.named_parameters():\n","   print(name, param.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmiefxfyjNrK","executionInfo":{"status":"ok","timestamp":1742841706869,"user_tz":-330,"elapsed":60,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"f64a5cec-3cbe-4b81-cf08-1062ca4b40f8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["bert.embeddings.word_embeddings.weight False\n","bert.embeddings.position_embeddings.weight False\n","bert.embeddings.token_type_embeddings.weight False\n","bert.embeddings.LayerNorm.weight False\n","bert.embeddings.LayerNorm.bias False\n","bert.encoder.layer.0.attention.self.query.weight False\n","bert.encoder.layer.0.attention.self.query.bias False\n","bert.encoder.layer.0.attention.self.key.weight False\n","bert.encoder.layer.0.attention.self.key.bias False\n","bert.encoder.layer.0.attention.self.value.weight False\n","bert.encoder.layer.0.attention.self.value.bias False\n","bert.encoder.layer.0.attention.output.dense.weight False\n","bert.encoder.layer.0.attention.output.dense.bias False\n","bert.encoder.layer.0.attention.output.LayerNorm.weight False\n","bert.encoder.layer.0.attention.output.LayerNorm.bias False\n","bert.encoder.layer.0.intermediate.dense.weight False\n","bert.encoder.layer.0.intermediate.dense.bias False\n","bert.encoder.layer.0.output.dense.weight False\n","bert.encoder.layer.0.output.dense.bias False\n","bert.encoder.layer.0.output.LayerNorm.weight False\n","bert.encoder.layer.0.output.LayerNorm.bias False\n","bert.encoder.layer.1.attention.self.query.weight False\n","bert.encoder.layer.1.attention.self.query.bias False\n","bert.encoder.layer.1.attention.self.key.weight False\n","bert.encoder.layer.1.attention.self.key.bias False\n","bert.encoder.layer.1.attention.self.value.weight False\n","bert.encoder.layer.1.attention.self.value.bias False\n","bert.encoder.layer.1.attention.output.dense.weight False\n","bert.encoder.layer.1.attention.output.dense.bias False\n","bert.encoder.layer.1.attention.output.LayerNorm.weight False\n","bert.encoder.layer.1.attention.output.LayerNorm.bias False\n","bert.encoder.layer.1.intermediate.dense.weight False\n","bert.encoder.layer.1.intermediate.dense.bias False\n","bert.encoder.layer.1.output.dense.weight False\n","bert.encoder.layer.1.output.dense.bias False\n","bert.encoder.layer.1.output.LayerNorm.weight False\n","bert.encoder.layer.1.output.LayerNorm.bias False\n","bert.encoder.layer.2.attention.self.query.weight False\n","bert.encoder.layer.2.attention.self.query.bias False\n","bert.encoder.layer.2.attention.self.key.weight False\n","bert.encoder.layer.2.attention.self.key.bias False\n","bert.encoder.layer.2.attention.self.value.weight False\n","bert.encoder.layer.2.attention.self.value.bias False\n","bert.encoder.layer.2.attention.output.dense.weight False\n","bert.encoder.layer.2.attention.output.dense.bias False\n","bert.encoder.layer.2.attention.output.LayerNorm.weight False\n","bert.encoder.layer.2.attention.output.LayerNorm.bias False\n","bert.encoder.layer.2.intermediate.dense.weight False\n","bert.encoder.layer.2.intermediate.dense.bias False\n","bert.encoder.layer.2.output.dense.weight False\n","bert.encoder.layer.2.output.dense.bias False\n","bert.encoder.layer.2.output.LayerNorm.weight False\n","bert.encoder.layer.2.output.LayerNorm.bias False\n","bert.encoder.layer.3.attention.self.query.weight False\n","bert.encoder.layer.3.attention.self.query.bias False\n","bert.encoder.layer.3.attention.self.key.weight False\n","bert.encoder.layer.3.attention.self.key.bias False\n","bert.encoder.layer.3.attention.self.value.weight False\n","bert.encoder.layer.3.attention.self.value.bias False\n","bert.encoder.layer.3.attention.output.dense.weight False\n","bert.encoder.layer.3.attention.output.dense.bias False\n","bert.encoder.layer.3.attention.output.LayerNorm.weight False\n","bert.encoder.layer.3.attention.output.LayerNorm.bias False\n","bert.encoder.layer.3.intermediate.dense.weight False\n","bert.encoder.layer.3.intermediate.dense.bias False\n","bert.encoder.layer.3.output.dense.weight False\n","bert.encoder.layer.3.output.dense.bias False\n","bert.encoder.layer.3.output.LayerNorm.weight False\n","bert.encoder.layer.3.output.LayerNorm.bias False\n","bert.encoder.layer.4.attention.self.query.weight False\n","bert.encoder.layer.4.attention.self.query.bias False\n","bert.encoder.layer.4.attention.self.key.weight False\n","bert.encoder.layer.4.attention.self.key.bias False\n","bert.encoder.layer.4.attention.self.value.weight False\n","bert.encoder.layer.4.attention.self.value.bias False\n","bert.encoder.layer.4.attention.output.dense.weight False\n","bert.encoder.layer.4.attention.output.dense.bias False\n","bert.encoder.layer.4.attention.output.LayerNorm.weight False\n","bert.encoder.layer.4.attention.output.LayerNorm.bias False\n","bert.encoder.layer.4.intermediate.dense.weight False\n","bert.encoder.layer.4.intermediate.dense.bias False\n","bert.encoder.layer.4.output.dense.weight False\n","bert.encoder.layer.4.output.dense.bias False\n","bert.encoder.layer.4.output.LayerNorm.weight False\n","bert.encoder.layer.4.output.LayerNorm.bias False\n","bert.encoder.layer.5.attention.self.query.weight False\n","bert.encoder.layer.5.attention.self.query.bias False\n","bert.encoder.layer.5.attention.self.key.weight False\n","bert.encoder.layer.5.attention.self.key.bias False\n","bert.encoder.layer.5.attention.self.value.weight False\n","bert.encoder.layer.5.attention.self.value.bias False\n","bert.encoder.layer.5.attention.output.dense.weight False\n","bert.encoder.layer.5.attention.output.dense.bias False\n","bert.encoder.layer.5.attention.output.LayerNorm.weight False\n","bert.encoder.layer.5.attention.output.LayerNorm.bias False\n","bert.encoder.layer.5.intermediate.dense.weight False\n","bert.encoder.layer.5.intermediate.dense.bias False\n","bert.encoder.layer.5.output.dense.weight False\n","bert.encoder.layer.5.output.dense.bias False\n","bert.encoder.layer.5.output.LayerNorm.weight False\n","bert.encoder.layer.5.output.LayerNorm.bias False\n","bert.encoder.layer.6.attention.self.query.weight False\n","bert.encoder.layer.6.attention.self.query.bias False\n","bert.encoder.layer.6.attention.self.key.weight False\n","bert.encoder.layer.6.attention.self.key.bias False\n","bert.encoder.layer.6.attention.self.value.weight False\n","bert.encoder.layer.6.attention.self.value.bias False\n","bert.encoder.layer.6.attention.output.dense.weight False\n","bert.encoder.layer.6.attention.output.dense.bias False\n","bert.encoder.layer.6.attention.output.LayerNorm.weight False\n","bert.encoder.layer.6.attention.output.LayerNorm.bias False\n","bert.encoder.layer.6.intermediate.dense.weight False\n","bert.encoder.layer.6.intermediate.dense.bias False\n","bert.encoder.layer.6.output.dense.weight False\n","bert.encoder.layer.6.output.dense.bias False\n","bert.encoder.layer.6.output.LayerNorm.weight False\n","bert.encoder.layer.6.output.LayerNorm.bias False\n","bert.encoder.layer.7.attention.self.query.weight False\n","bert.encoder.layer.7.attention.self.query.bias False\n","bert.encoder.layer.7.attention.self.key.weight False\n","bert.encoder.layer.7.attention.self.key.bias False\n","bert.encoder.layer.7.attention.self.value.weight False\n","bert.encoder.layer.7.attention.self.value.bias False\n","bert.encoder.layer.7.attention.output.dense.weight False\n","bert.encoder.layer.7.attention.output.dense.bias False\n","bert.encoder.layer.7.attention.output.LayerNorm.weight False\n","bert.encoder.layer.7.attention.output.LayerNorm.bias False\n","bert.encoder.layer.7.intermediate.dense.weight False\n","bert.encoder.layer.7.intermediate.dense.bias False\n","bert.encoder.layer.7.output.dense.weight False\n","bert.encoder.layer.7.output.dense.bias False\n","bert.encoder.layer.7.output.LayerNorm.weight False\n","bert.encoder.layer.7.output.LayerNorm.bias False\n","bert.encoder.layer.8.attention.self.query.weight False\n","bert.encoder.layer.8.attention.self.query.bias False\n","bert.encoder.layer.8.attention.self.key.weight False\n","bert.encoder.layer.8.attention.self.key.bias False\n","bert.encoder.layer.8.attention.self.value.weight False\n","bert.encoder.layer.8.attention.self.value.bias False\n","bert.encoder.layer.8.attention.output.dense.weight False\n","bert.encoder.layer.8.attention.output.dense.bias False\n","bert.encoder.layer.8.attention.output.LayerNorm.weight False\n","bert.encoder.layer.8.attention.output.LayerNorm.bias False\n","bert.encoder.layer.8.intermediate.dense.weight False\n","bert.encoder.layer.8.intermediate.dense.bias False\n","bert.encoder.layer.8.output.dense.weight False\n","bert.encoder.layer.8.output.dense.bias False\n","bert.encoder.layer.8.output.LayerNorm.weight False\n","bert.encoder.layer.8.output.LayerNorm.bias False\n","bert.encoder.layer.9.attention.self.query.weight False\n","bert.encoder.layer.9.attention.self.query.bias False\n","bert.encoder.layer.9.attention.self.key.weight False\n","bert.encoder.layer.9.attention.self.key.bias False\n","bert.encoder.layer.9.attention.self.value.weight False\n","bert.encoder.layer.9.attention.self.value.bias False\n","bert.encoder.layer.9.attention.output.dense.weight False\n","bert.encoder.layer.9.attention.output.dense.bias False\n","bert.encoder.layer.9.attention.output.LayerNorm.weight False\n","bert.encoder.layer.9.attention.output.LayerNorm.bias False\n","bert.encoder.layer.9.intermediate.dense.weight False\n","bert.encoder.layer.9.intermediate.dense.bias False\n","bert.encoder.layer.9.output.dense.weight False\n","bert.encoder.layer.9.output.dense.bias False\n","bert.encoder.layer.9.output.LayerNorm.weight False\n","bert.encoder.layer.9.output.LayerNorm.bias False\n","bert.encoder.layer.10.attention.self.query.weight False\n","bert.encoder.layer.10.attention.self.query.bias False\n","bert.encoder.layer.10.attention.self.key.weight False\n","bert.encoder.layer.10.attention.self.key.bias False\n","bert.encoder.layer.10.attention.self.value.weight False\n","bert.encoder.layer.10.attention.self.value.bias False\n","bert.encoder.layer.10.attention.output.dense.weight False\n","bert.encoder.layer.10.attention.output.dense.bias False\n","bert.encoder.layer.10.attention.output.LayerNorm.weight False\n","bert.encoder.layer.10.attention.output.LayerNorm.bias False\n","bert.encoder.layer.10.intermediate.dense.weight False\n","bert.encoder.layer.10.intermediate.dense.bias False\n","bert.encoder.layer.10.output.dense.weight False\n","bert.encoder.layer.10.output.dense.bias False\n","bert.encoder.layer.10.output.LayerNorm.weight False\n","bert.encoder.layer.10.output.LayerNorm.bias False\n","bert.encoder.layer.11.attention.self.query.weight False\n","bert.encoder.layer.11.attention.self.query.bias False\n","bert.encoder.layer.11.attention.self.key.weight False\n","bert.encoder.layer.11.attention.self.key.bias False\n","bert.encoder.layer.11.attention.self.value.weight False\n","bert.encoder.layer.11.attention.self.value.bias False\n","bert.encoder.layer.11.attention.output.dense.weight False\n","bert.encoder.layer.11.attention.output.dense.bias False\n","bert.encoder.layer.11.attention.output.LayerNorm.weight False\n","bert.encoder.layer.11.attention.output.LayerNorm.bias False\n","bert.encoder.layer.11.intermediate.dense.weight False\n","bert.encoder.layer.11.intermediate.dense.bias False\n","bert.encoder.layer.11.output.dense.weight False\n","bert.encoder.layer.11.output.dense.bias False\n","bert.encoder.layer.11.output.LayerNorm.weight False\n","bert.encoder.layer.11.output.LayerNorm.bias False\n","bert.pooler.dense.weight True\n","bert.pooler.dense.bias True\n","classifier.weight True\n","classifier.bias True\n"]}]},{"cell_type":"markdown","source":["# Text Preprocessing Function\n","\n","This cell defines a simple preprocessing function using the tokenizer to prepare text data for input into the model.\n","\n"],"metadata":{"id":"b7OdwcKQmFVR"}},{"cell_type":"code","source":["# define text preprocessing\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)"],"metadata":{"id":"FSMkoFKfjPYU","executionInfo":{"status":"ok","timestamp":1742841707895,"user_tz":-330,"elapsed":16,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Tokenizing All Datasets\n","\n","This cell applies the previously defined `preprocess_function` to tokenize all the datasets using the `map` function from the Hugging Face Datasets library.\n","\n"],"metadata":{"id":"exAtwuVlmONJ"}},{"cell_type":"code","source":["# tokenize all datasetse\n","tokenized_data = dataset_dict.map(preprocess_function, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["738fa62babce4ef180835da37c4c2aa7","745c414042594ea1896f927193077689","0d10521c53e045b58ef8b5956159b6ac","a40677382d494dbcb58325df2fd4665e","d832f73454b84e4182ceff611c351ce9","5c1ad1df69ff40768951cf62e11c6cb5","5372ffd17a3444448529f74df8909b5e","47c1543027b34720bfeb4f8c8d5b6ced","dc7701b5e3f9490098cf217b8009f2eb","d45df5d878604b88a74bff6cf6b1e357","ccfde8bace4244ab9e5fbc3de244e0a1"]},"id":"cKab6rnUjSkI","executionInfo":{"status":"ok","timestamp":1742841709694,"user_tz":-330,"elapsed":295,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"94a9471c-c1c6-4761-f787-1bf17985977a"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/450 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738fa62babce4ef180835da37c4c2aa7"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Creating a Data Collator\n","\n","This cell creates a **Data Collator** using the `DataCollatorWithPadding` class from Hugging Face Transformers.\n","\n"],"metadata":{"id":"Y7LA0kYpmaBY"}},{"cell_type":"code","source":["# create data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"Ex0Tha2yjUSS","executionInfo":{"status":"ok","timestamp":1742841713649,"user_tz":-330,"elapsed":5,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Loading Metrics and Computing Evaluation Metrics\n","\n","This cell is responsible for evaluating the model's performance using accuracy and AUC (Area Under the Curve) metrics.\n","\n"],"metadata":{"id":"4LfPPbRgm14h"}},{"cell_type":"code","source":["# load metrics\n","accuracy = evaluate.load(\"accuracy\")\n","auc_score = evaluate.load(\"roc_auc\")\n","\n","def compute_metrics(eval_pred):\n","    # get predictions\n","    predictions, labels = eval_pred\n","\n","    # apply softmax to get probabilities\n","    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n","    # use probabilities of the positive class for ROC AUC\n","    positive_class_probs = probabilities[:, 1]\n","    # compute auc\n","    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs, references=labels)['roc_auc'],3)\n","\n","    # predict most probable class\n","    predicted_classes = np.argmax(predictions, axis=1)\n","    # compute accuracy\n","    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n","\n","    return {\"Accuracy\": acc, \"AUC\": auc}"],"metadata":{"id":"h7r0ebJSjW0Z","executionInfo":{"status":"ok","timestamp":1742841715798,"user_tz":-330,"elapsed":976,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Explanation of Hyperparameters and TrainingArguments\n","\n","This cell defines the hyperparameters for the model training process and sets up the training configuration using the `TrainingArguments` class from the Hugging Face Transformers library.\n"],"metadata":{"id":"P9OAl_aFnBqP"}},{"cell_type":"code","source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 8\n","num_epochs = 10\n","\n","training_args = TrainingArguments(\n","    output_dir=\"bert-phishing-classifier_teacher\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    logging_strategy=\"epoch\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"],"metadata":{"id":"T3xUGW4VjY-a","executionInfo":{"status":"ok","timestamp":1742841717760,"user_tz":-330,"elapsed":88,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Emissions Tracking and Model Training with CodeCarbon\n","\n","This cell performs two key functions:  \n","1. **Training the Model**: It uses the Hugging Face `Trainer` class to fine-tune the BERT model on the phishing site classification dataset.  \n","2. **Tracking Emissions**: The `EmissionsTracker` from CodeCarbon monitors and reports the carbon footprint generated during the training process.\n"],"metadata":{"id":"WOjTrD66nMw0"}},{"cell_type":"code","source":["from codecarbon import EmissionsTracker\n","import pandas as pd\n","from transformers import Trainer\n","\n","# Assuming trainer is already set up\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data[\"train\"],\n","    eval_dataset=tokenized_data[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train the model with emissions tracking\n","with EmissionsTracker(output_dir='./', output_file='emissions.csv', allow_multiple_runs=True) as tracker:\n","    trainer.train()\n","\n","# Read the emissions data from the CSV file\n","emissions_df = pd.read_csv('emissions.csv')\n","latest_emissions = emissions_df['emissions'].iloc[-1]\n","print(f\"Total emissions: {latest_emissions} kgCO2eq\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3cR5x1WUjcEZ","outputId":"af53ee8f-d970-4952-9de2-9ca0776d5032","executionInfo":{"status":"ok","timestamp":1742842291430,"user_tz":-330,"elapsed":167447,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-1a76bebd0920>:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","[codecarbon ERROR @ 18:48:44] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n","[codecarbon WARNING @ 18:48:44] Multiple instances of codecarbon are allowed to run at the same time.\n","[codecarbon INFO @ 18:48:44] [setup] RAM Tracking...\n","[codecarbon INFO @ 18:48:44] [setup] CPU Tracking...\n","[codecarbon WARNING @ 18:48:44] No CPU tracking mode found. Falling back on CPU constant mode. \n"," Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n","\n","[codecarbon WARNING @ 18:48:45] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n","[codecarbon INFO @ 18:48:45] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n","[codecarbon INFO @ 18:48:45] [setup] GPU Tracking...\n","[codecarbon INFO @ 18:48:45] Tracking Nvidia GPU via pynvml\n","[codecarbon INFO @ 18:48:45] >>> Tracker's metadata:\n","[codecarbon INFO @ 18:48:45]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n","[codecarbon INFO @ 18:48:45]   Python version: 3.11.11\n","[codecarbon INFO @ 18:48:45]   CodeCarbon version: 2.8.3\n","[codecarbon INFO @ 18:48:45]   Available RAM : 12.675 GB\n","[codecarbon INFO @ 18:48:45]   CPU count: 2\n","[codecarbon INFO @ 18:48:45]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n","[codecarbon INFO @ 18:48:45]   GPU count: 1\n","[codecarbon INFO @ 18:48:45]   GPU model: 1 x Tesla T4\n","[codecarbon INFO @ 18:48:45] Saving emissions data to file /content/emissions.csv\n","[codecarbon WARNING @ 18:48:46] Another instance of codecarbon is already running. Exiting.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2630/2630 02:45, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Auc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.332000</td>\n","      <td>0.275443</td>\n","      <td>0.873000</td>\n","      <td>0.954000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.324800</td>\n","      <td>0.280714</td>\n","      <td>0.887000</td>\n","      <td>0.953000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.297000</td>\n","      <td>0.284749</td>\n","      <td>0.882000</td>\n","      <td>0.953000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.313500</td>\n","      <td>0.323642</td>\n","      <td>0.876000</td>\n","      <td>0.954000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.313700</td>\n","      <td>0.293310</td>\n","      <td>0.882000</td>\n","      <td>0.956000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.315500</td>\n","      <td>0.271854</td>\n","      <td>0.882000</td>\n","      <td>0.956000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.308600</td>\n","      <td>0.273879</td>\n","      <td>0.871000</td>\n","      <td>0.955000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.293200</td>\n","      <td>0.275373</td>\n","      <td>0.873000</td>\n","      <td>0.955000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.295600</td>\n","      <td>0.270155</td>\n","      <td>0.876000</td>\n","      <td>0.956000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.301300</td>\n","      <td>0.276364</td>\n","      <td>0.873000</td>\n","      <td>0.956000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[codecarbon INFO @ 18:49:00] Energy consumed for RAM : 0.000020 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:49:00] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:49:00] Energy consumed for all GPUs : 0.000253 kWh. Total GPU Power : 60.57242975746068 W\n","[codecarbon INFO @ 18:49:00] 0.000450 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:49:17] Energy consumed for RAM : 0.000043 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:49:17] Energy consumed for all CPUs : 0.000380 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:49:17] Energy consumed for all GPUs : 0.000528 kWh. Total GPU Power : 57.611011541919915 W\n","[codecarbon INFO @ 18:49:17] 0.000950 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:49:32] Energy consumed for RAM : 0.000062 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:49:32] Energy consumed for all CPUs : 0.000557 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:49:32] Energy consumed for all GPUs : 0.000793 kWh. Total GPU Power : 63.64548618746113 W\n","[codecarbon INFO @ 18:49:32] 0.001412 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:49:47] Energy consumed for RAM : 0.000082 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:49:47] Energy consumed for all CPUs : 0.000734 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:49:47] Energy consumed for all GPUs : 0.001029 kWh. Total GPU Power : 56.719790466374526 W\n","[codecarbon INFO @ 18:49:47] 0.001845 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:50:02] Energy consumed for RAM : 0.000102 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:50:02] Energy consumed for all CPUs : 0.000911 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:50:02] Energy consumed for all GPUs : 0.001299 kWh. Total GPU Power : 64.76986970126613 W\n","[codecarbon INFO @ 18:50:02] 0.002312 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:50:17] Energy consumed for RAM : 0.000122 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:50:17] Energy consumed for all CPUs : 0.001088 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:50:17] Energy consumed for all GPUs : 0.001568 kWh. Total GPU Power : 64.7376934182975 W\n","[codecarbon INFO @ 18:50:17] 0.002778 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:50:32] Energy consumed for RAM : 0.000141 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:50:32] Energy consumed for all CPUs : 0.001265 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:50:32] Energy consumed for all GPUs : 0.001769 kWh. Total GPU Power : 48.137946071713785 W\n","[codecarbon INFO @ 18:50:32] 0.003175 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:50:48] Energy consumed for RAM : 0.000162 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:50:48] Energy consumed for all CPUs : 0.001453 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:50:48] Energy consumed for all GPUs : 0.002026 kWh. Total GPU Power : 58.192721277631755 W\n","[codecarbon INFO @ 18:50:48] 0.003641 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:50:48] 0.010325 g.CO2eq/s mean an estimation of 325.6202103583896 kg.CO2eq/year\n","[codecarbon INFO @ 18:51:04] Energy consumed for RAM : 0.000183 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:51:04] Energy consumed for all CPUs : 0.001640 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:51:04] Energy consumed for all GPUs : 0.002283 kWh. Total GPU Power : 58.30459382247707 W\n","[codecarbon INFO @ 18:51:04] 0.004106 kWh of electricity used since the beginning.\n","[codecarbon INFO @ 18:51:21] Energy consumed for RAM : 0.000206 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:51:21] Energy consumed for all CPUs : 0.001839 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:51:21] Energy consumed for all GPUs : 0.002530 kWh. Total GPU Power : 52.81071416049271 W\n","[codecarbon INFO @ 18:51:21] 0.004574 kWh of electricity used since the beginning.\n","[codecarbon WARNING @ 18:51:31] Another instance of codecarbon is already running. Exiting.\n","[codecarbon INFO @ 18:51:31] Energy consumed for RAM : 0.000219 kWh. RAM Power : 4.7530388832092285 W\n","[codecarbon INFO @ 18:51:31] Energy consumed for all CPUs : 0.001955 kWh. Total CPU Power : 42.5 W\n","[codecarbon INFO @ 18:51:31] Energy consumed for all GPUs : 0.002635 kWh. Total GPU Power : 38.33806325816436 W\n","[codecarbon INFO @ 18:51:31] 0.004809 kWh of electricity used since the beginning.\n"]},{"output_type":"stream","name":"stdout","text":["Total emissions: 0.0016795223142456 kgCO2eq\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"]}]},{"cell_type":"markdown","source":["# Validation and Emissions Evaluation\n","\n","This cell performs the following tasks:  \n","1. **Model Validation**: Applies the trained BERT model to the validation dataset and evaluates its performance.  \n","2. **Metrics Calculation**: Computes accuracy and AUC using the `compute_metrics()` function.  \n","3. **Emissions Reporting**: Reads and prints the total carbon emissions generated during training using the data stored in the `emissions.csv` file.  \n"],"metadata":{"id":"QY0HctnmnUS4"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Apply model to validation dataset\n","predictions = trainer.predict(tokenized_data[\"validation\"])\n","\n","# Extract the logits and labels from the predictions object\n","logits = predictions.predictions\n","labels = predictions.label_ids\n","\n","# Use your compute_metrics function\n","metrics = compute_metrics((logits, labels))\n","\n","# Read the total emissions from the CSV file generated during training\n","emissions_df = pd.read_csv('emissions.csv')\n","total_emissions = emissions_df['emissions'].iloc[-1]\n","\n","# Print the validation metrics and total emissions together\n","print(\"Validation Metrics and Total Emissions:\")\n","print(f\"Metrics: {metrics}\")\n","print(f\"Total emissions: {total_emissions} kgCO2eq\")"],"metadata":{"id":"r2au-UXPjeJF","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1742842677961,"user_tz":-330,"elapsed":1923,"user":{"displayName":"Yash Bhatter","userId":"14481163759016320843"}},"outputId":"86a17674-69b8-459b-ba29-95efbcc7fd9d"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation Metrics and Total Emissions:\n","Metrics: {'Accuracy': np.float64(0.902), 'AUC': np.float64(0.949)}\n","Total emissions: 0.0016795223142456 kgCO2eq\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"I4_nGLCihMH8"},"execution_count":null,"outputs":[]}]}